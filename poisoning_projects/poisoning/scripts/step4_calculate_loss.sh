#!/usr/bin/env bash
set -euo pipefail

# Step 4: Calculate loss for all datasets
# This script calculates loss on poisoned datasets generated by Step 3

# Load common environment
source "$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)/env.sh"

# Configuration from presets
POISONING_PERCENTAGES=("${all_POISONING_PERCENTAGES[@]}")
POISONING_PERCENTAGES_consecutive=("${all_POISONING_PERCENTAGES_consecutive[@]}")
real_dataset_names=("${all_real_dataset_names[@]}")
sync_dataset_names=("${all_sync_dataset_names[@]}")
ns=("${all_ns[@]}")
ns_consecutive=("${all_ns_consecutive[@]}")
seeds=("${all_seeds[@]}")
Rs=("${all_Rs[@]}")
Rs_consecutive=("${all_Rs_consecutive[@]}")
brute_force_POISONING_PERCENTAGES=("${all_brute_force_POISONING_PERCENTAGES[@]}")
brute_force_ns=("${all_brute_force_ns[@]}")
brute_force_Rs=("${all_brute_force_Rs[@]}")

# Parse command line arguments
ALL_MODE=false
QUICK_MODE=false

while [[ $# -gt 0 ]]; do
    case $1 in
        --all)
            ALL_MODE=true
            shift
            ;;
        --quick)
            QUICK_MODE=true
            POISONING_PERCENTAGES=("${quick_POISONING_PERCENTAGES[@]}")
            real_dataset_names=("${quick_real_dataset_names[@]}")
            sync_dataset_names=("${quick_sync_dataset_names[@]}")
            ns=("${quick_ns[@]}")
            seeds=("${quick_seeds[@]}")
            Rs=("${quick_Rs[@]}")
            brute_force_POISONING_PERCENTAGES=("${quick_brute_force_POISONING_PERCENTAGES[@]}")
            brute_force_ns=("${quick_brute_force_ns[@]}")
            brute_force_Rs=("${quick_brute_force_Rs[@]}")
            shift
            ;;
        *)
            echo "Unknown option: $1"
            echo "Usage: $0 --all|--quick"
            exit 1
            ;;
    esac
done

# Ensure mode specified
if [ "$ALL_MODE" = false ] && [ "$QUICK_MODE" = false ]; then
    echo "Error: Either --all or --quick option must be specified"
    echo "Usage: $0 --all|--quick"
    exit 1
fi

echo "Step 4: Calculating loss for poisoned datasets..."
echo "  Data directory: $DATA_DIR"
echo "  Results directory: $RESULTS_DIR"
echo "  Base poisoning percentage: $base_POISONING_PERCENTAGE"
echo "  Base n: $base_n"
echo "  Base R: $base_R"
echo ""

# Prepare results directory
mkdir -p "$RESULTS_DIR/loss"
mkdir -p "$RESULTS_DIR/loss_consecutive"
mkdir -p "$RESULTS_DIR/loss_consecutive_w_endpoints"
mkdir -p "$RESULTS_DIR/loss_duplicate_allowed"
mkdir -p "$RESULTS_DIR/loss_consecutive_w_endpoints_using_relaxed_solution"

# Move to build directory to run binaries
cd "$BUILD_DIR"

# Function to calculate loss for a dataset
run_calc_loss() {
    local input_file_name="$1"
    local poison_num="$2"
    local base_output_name="$3"
    local dataset_name="$4"
    local n_value="$5"

    if [ ! -f "$DATA_DIR/${input_file_name}" ]; then
        echo "    Input file not found: $DATA_DIR/${input_file_name}"
        return
    fi

    local out_dir="$RESULTS_DIR/loss/${dataset_name}/n${n_value}/lambda${poison_num}"
    local loss_json_file="$out_dir/${base_output_name}_loss.json"
    mkdir -p "$out_dir"

    if [ -f "$loss_json_file" ]; then
        echo "    [Skipped] Output JSON already exists: ${base_output_name}_loss.json"
        return
    fi

    echo "    Calculating loss for: $input_file_name (lambda=$poison_num)"
    if ./calc_loss "$DATA_DIR/${input_file_name}" "$loss_json_file" 2>/dev/null; then
        echo "      Loss calculation completed"
    else
        echo "      [ERROR] Error calculating loss for $input_file_name"
        exit 1
    fi
}

# Function to calculate loss for a dataset (consecutive approach)
run_calc_loss_consecutive() {
    local input_file_name="$1"
    local poison_num="$2"
    local base_output_name="$3"
    local dataset_name="$4"
    local n_value="$5"

    if [ ! -f "$DATA_DIR/${input_file_name}" ]; then
        echo "    Input file not found: $DATA_DIR/${input_file_name}"
        return
    fi

    local out_dir="$RESULTS_DIR/loss_consecutive/${dataset_name}/n${n_value}/lambda${poison_num}"
    local loss_json_file="$out_dir/${base_output_name}_loss.json"
    mkdir -p "$out_dir"

    if [ -f "$loss_json_file" ]; then
        echo "    [Skipped] Output JSON already exists: ${base_output_name}_loss.json"
        return
    fi

    echo "    Calculating loss for: $input_file_name (lambda=$poison_num, consecutive)"
    if ./calc_loss "$DATA_DIR/${input_file_name}" "$loss_json_file" 2>/dev/null; then
        echo "      Loss calculation completed"
    else
        echo "      [ERROR] Error calculating loss for $input_file_name"
        exit 1
    fi
}

# Function to calculate loss for a dataset (consecutive with endpoints approach)
run_calc_loss_consecutive_w_endpoints() {
    local input_file_name="$1"
    local poison_num="$2"
    local base_output_name="$3"
    local dataset_name="$4"
    local n_value="$5"

    if [ ! -f "$DATA_DIR/${input_file_name}" ]; then
        echo "    Input file not found: $DATA_DIR/${input_file_name}"
        return
    fi

    local out_dir="$RESULTS_DIR/loss_consecutive_w_endpoints/${dataset_name}/n${n_value}/lambda${poison_num}"
    local loss_json_file="$out_dir/${base_output_name}_loss.json"
    mkdir -p "$out_dir"

    if [ -f "$loss_json_file" ]; then
        echo "    [Skipped] Output JSON already exists: ${base_output_name}_loss.json"
        return
    fi

    echo "    Calculating loss for: $input_file_name (lambda=$poison_num, consecutive with endpoints)"
    if ./calc_loss "$DATA_DIR/${input_file_name}" "$loss_json_file" 2>/dev/null; then
        echo "      Loss calculation completed"
    else
        echo "      [ERROR] Error calculating loss for $input_file_name"
        exit 1
    fi
}

# Function to calculate loss for a dataset (consecutive with endpoints approach (duplicate allowed))
run_calc_loss_consecutive_w_endpoints_duplicate_allowed() {
    local input_file_name="$1"
    local poison_num="$2"
    local base_output_name="$3"
    local dataset_name="$4"
    local n_value="$5"

    if [ ! -f "$DATA_DIR/${input_file_name}" ]; then
        echo "    Input file not found: $DATA_DIR/${input_file_name}"
        return
    fi

    local out_dir="$RESULTS_DIR/loss_consecutive_w_endpoints_duplicate_allowed/${dataset_name}/n${n_value}/lambda${poison_num}"
    local loss_json_file="$out_dir/${base_output_name}_loss.json"
    mkdir -p "$out_dir"

    if [ -f "$loss_json_file" ]; then
        echo "    [Skipped] Output JSON already exists: ${base_output_name}_loss.json"
        return
    fi

    echo "    Calculating loss for: $input_file_name (lambda=$poison_num, consecutive with endpoints duplicate allowed)"
    if ./calc_loss "$DATA_DIR/${input_file_name}" "$loss_json_file" 2>/dev/null; then
        echo "      Loss calculation completed"
    else
        echo "      [ERROR] Error calculating loss for $input_file_name"
        exit 1
    fi
}

# Function to calculate loss for a dataset (consecutive with endpoints approach (using relaxed solution))
run_calc_loss_consecutive_w_endpoints_using_relaxed_solution() {
    local input_file_name="$1"
    local poison_num="$2"
    local base_output_name="$3"
    local dataset_name="$4"
    local n_value="$5"

    if [ ! -f "$DATA_DIR/${input_file_name}" ]; then
        echo "    Input file not found: $DATA_DIR/${input_file_name}"
        return
    fi

    local out_dir="$RESULTS_DIR/loss_consecutive_w_endpoints_using_relaxed_solution/${dataset_name}/n${n_value}/lambda${poison_num}"
    local loss_json_file="$out_dir/${base_output_name}_loss.json"
    mkdir -p "$out_dir"

    if [ -f "$loss_json_file" ]; then
        echo "    [Skipped] Output JSON already exists: ${base_output_name}_loss.json"
        return
    fi

    echo "    Calculating loss for: $input_file_name (lambda=$poison_num, consecutive with endpoints using relaxed solution)"
    if ./calc_loss "$DATA_DIR/${input_file_name}" "$loss_json_file" 2>/dev/null; then
        echo "      Loss calculation completed"
    else
        echo "      [ERROR] Error calculating loss for $input_file_name"
        exit 1
    fi
}

# Function to calculate loss for a dataset (duplicate allowed approach)
run_calc_loss_duplicate_allowed() {
    local input_file_name="$1"
    local poison_num="$2"
    local base_output_name="$3"
    local dataset_name="$4"
    local n_value="$5"

    if [ ! -f "$DATA_DIR/${input_file_name}" ]; then
        echo "    Input file not found: $DATA_DIR/${input_file_name}"
        return
    fi

    local out_dir="$RESULTS_DIR/loss_duplicate_allowed/${dataset_name}/n${n_value}/lambda${poison_num}"
    local loss_json_file="$out_dir/${base_output_name}_loss.json"
    mkdir -p "$out_dir"

    if [ -f "$loss_json_file" ]; then
        echo "    [Skipped] Output JSON already exists: ${base_output_name}_loss.json"
        return
    fi

    echo "    Calculating loss for: $input_file_name (lambda=$poison_num, duplicate allowed)"
    if ./calc_loss "$DATA_DIR/${input_file_name}" "$loss_json_file" 2>/dev/null; then
        echo "      Loss calculation completed"
    else
        echo "      [ERROR] Error calculating loss for $input_file_name"
        exit 1
    fi
}

# Function to process real datasets with given parameters
process_real_datasets_loss() {
    local n_val="$1"
    local percentage="$2"
    local poison_num=$((n_val * percentage / 100))
    
    for real_dataset_name in "${real_dataset_names[@]}"; do
        for seed in "${seeds[@]}"; do
            for dtype in "uint64"; do
                # Calculate loss for legitimate datasets (only for base parameters)
                if [ "$n_val" = "$base_n" ] && [ "$percentage" = "$base_POISONING_PERCENTAGE" ]; then
                    echo "Processing: $real_dataset_name n=$n_val seed=$seed dtype=$dtype (legitimate)"
                    input_file_name="${real_dataset_name}_n${n_val}_seed${seed}_${dtype}"
                    base_output_name="${real_dataset_name}_n${n_val}_seed${seed}_${dtype}"
                    run_calc_loss "$input_file_name" "0" "$base_output_name" "$real_dataset_name" "$n_val"
                fi
                
                # Calculate loss for poisoned datasets
                echo "Processing: $real_dataset_name n=$n_val seed=$seed dtype=$dtype percentage=$percentage"
                input_file_name="${real_dataset_name}_n${n_val}_seed${seed}_lambda${poison_num}_${dtype}"
                base_output_name="${real_dataset_name}_n${n_val}_seed${seed}_lambda${poison_num}_percentage${percentage}_${dtype}"
                run_calc_loss "$input_file_name" "$poison_num" "$base_output_name" "$real_dataset_name" "$n_val"
            done
        done
    done
}

# Function to process synthetic datasets with given parameters
process_sync_datasets_loss() {
    local n_val="$1"
    local R_val="$2"
    local percentage="$3"
    local poison_num=$((n_val * percentage / 100))
    
    for sync_dataset_name in "${sync_dataset_names[@]}"; do
        for seed in "${seeds[@]}"; do
            for dtype in "uint64"; do
                # Calculate loss for legitimate datasets (only for base parameters)
                if [ "$n_val" = "$base_n" ] && [ "$R_val" = "$base_R" ] && [ "$percentage" = "$base_POISONING_PERCENTAGE" ]; then
                    echo "Processing: $sync_dataset_name n=$n_val R=$R_val seed=$seed dtype=$dtype (legitimate)"
                    input_file_name="${sync_dataset_name}_n${n_val}_R${R_val}_seed${seed}_${dtype}"
                    base_output_name="${sync_dataset_name}_n${n_val}_R${R_val}_seed${seed}_${dtype}"
                    run_calc_loss "$input_file_name" "0" "$base_output_name" "$sync_dataset_name" "$n_val"
                fi
                
                # Calculate loss for poisoned datasets
                echo "Processing: $sync_dataset_name n=$n_val R=$R_val seed=$seed dtype=$dtype percentage=$percentage"
                input_file_name="${sync_dataset_name}_n${n_val}_R${R_val}_seed${seed}_lambda${poison_num}_${dtype}"
                base_output_name="${sync_dataset_name}_n${n_val}_R${R_val}_seed${seed}_lambda${poison_num}_percentage${percentage}_${dtype}"
                run_calc_loss "$input_file_name" "$poison_num" "$base_output_name" "$sync_dataset_name" "$n_val"
            done
        done
    done
}

# Function to process real datasets with given parameters (consecutive approach)
process_real_datasets_consecutive_loss() {
    local n_val="$1"
    local percentage="$2"
    local poison_num=$((n_val * percentage / 100))
    
    for real_dataset_name in "${real_dataset_names[@]}"; do
        for seed in "${seeds[@]}"; do
            for dtype in "uint64"; do
                # Calculate loss for poisoned datasets using consecutive approach
                echo "Processing: $real_dataset_name n=$n_val seed=$seed dtype=$dtype percentage=$percentage (consecutive)"
                input_file_name="${real_dataset_name}_n${n_val}_seed${seed}_lambda${poison_num}_consecutive_${dtype}"
                base_output_name="${real_dataset_name}_n${n_val}_seed${seed}_lambda${poison_num}_consecutive_percentage${percentage}_${dtype}"
                run_calc_loss_consecutive "$input_file_name" "$poison_num" "$base_output_name" "$real_dataset_name" "$n_val"
            done
        done
    done
}

# Function to process real datasets with given parameters (consecutive with endpoints approach)
process_real_datasets_consecutive_w_endpoints_loss() {
    local n_val="$1"
    local percentage="$2"
    local poison_num=$((n_val * percentage / 100))
    
    for real_dataset_name in "${real_dataset_names[@]}"; do
        for seed in "${seeds[@]}"; do
            for dtype in "uint64"; do
                # Calculate loss for poisoned datasets using consecutive with endpoints approach
                echo "Processing: $real_dataset_name n=$n_val seed=$seed dtype=$dtype percentage=$percentage (consecutive with endpoints)"
                input_file_name="${real_dataset_name}_n${n_val}_seed${seed}_lambda${poison_num}_consecutive_w_endpoints_${dtype}"
                base_output_name="${real_dataset_name}_n${n_val}_seed${seed}_lambda${poison_num}_consecutive_w_endpoints_percentage${percentage}_${dtype}"
                run_calc_loss_consecutive_w_endpoints "$input_file_name" "$poison_num" "$base_output_name" "$real_dataset_name" "$n_val"
            done
        done
    done
}

# Function to process synthetic datasets with given parameters (consecutive approach)
process_sync_datasets_consecutive_loss() {
    local n_val="$1"
    local R_val="$2"
    local percentage="$3"
    local poison_num=$((n_val * percentage / 100))
    
    for sync_dataset_name in "${sync_dataset_names[@]}"; do
        for seed in "${seeds[@]}"; do
            for dtype in "uint64"; do
                # Calculate loss for poisoned datasets using consecutive approach
                echo "Processing: $sync_dataset_name n=$n_val R=$R_val seed=$seed dtype=$dtype percentage=$percentage (consecutive)"
                input_file_name="${sync_dataset_name}_n${n_val}_R${R_val}_seed${seed}_lambda${poison_num}_consecutive_${dtype}"
                base_output_name="${sync_dataset_name}_n${n_val}_R${R_val}_seed${seed}_lambda${poison_num}_consecutive_percentage${percentage}_${dtype}"
                run_calc_loss_consecutive "$input_file_name" "$poison_num" "$base_output_name" "$sync_dataset_name" "$n_val"
            done
        done
    done
}

# Function to process synthetic datasets with given parameters (consecutive with endpoints approach)
process_sync_datasets_consecutive_w_endpoints_loss() {
    local n_val="$1"
    local R_val="$2"
    local percentage="$3"
    local poison_num=$((n_val * percentage / 100))
    
    for sync_dataset_name in "${sync_dataset_names[@]}"; do
        for seed in "${seeds[@]}"; do
            for dtype in "uint64"; do
                # Calculate loss for poisoned datasets using consecutive with endpoints approach
                echo "Processing: $sync_dataset_name n=$n_val R=$R_val seed=$seed dtype=$dtype percentage=$percentage (consecutive with endpoints)"
                input_file_name="${sync_dataset_name}_n${n_val}_R${R_val}_seed${seed}_lambda${poison_num}_consecutive_w_endpoints_${dtype}"
                base_output_name="${sync_dataset_name}_n${n_val}_R${R_val}_seed${seed}_lambda${poison_num}_consecutive_w_endpoints_percentage${percentage}_${dtype}"
                run_calc_loss_consecutive_w_endpoints "$input_file_name" "$poison_num" "$base_output_name" "$sync_dataset_name" "$n_val"
            done
        done
    done
}

# Function to process synthetic datasets with given parameters (consecutive with endpoints approach (duplicate allowed))
process_sync_datasets_consecutive_w_endpoints_duplicate_allowed_loss() {
    local n_val="$1"
    local R_val="$2"
    local percentage="$3"
    local poison_num=$((n_val * percentage / 100))

    for sync_dataset_name in "${sync_dataset_names[@]}"; do
        for seed in "${seeds[@]}"; do
            for dtype in "uint64"; do
                # Calculate loss for poisoned datasets using consecutive with endpoints approach (duplicate allowed)
                echo "Processing: $sync_dataset_name n=$n_val R=$R_val seed=$seed dtype=$dtype percentage=$percentage (consecutive with endpoints duplicate allowed)"
                input_file_name="${sync_dataset_name}_n${n_val}_R${R_val}_seed${seed}_lambda${poison_num}_consecutive_w_endpoints_duplicate_allowed_${dtype}"
                base_output_name="${sync_dataset_name}_n${n_val}_R${R_val}_seed${seed}_lambda${poison_num}_consecutive_w_endpoints_duplicate_allowed_percentage${percentage}_${dtype}"
                run_calc_loss_consecutive_w_endpoints_duplicate_allowed "$input_file_name" "$poison_num" "$base_output_name" "$sync_dataset_name" "$n_val"
            done
        done
    done
}

# Function to process synthetic datasets with given parameters (consecutive with endpoints approach (using relaxed solution))
process_sync_datasets_consecutive_w_endpoints_using_relaxed_solution_loss() {
    local n_val="$1"
    local R_val="$2"
    local percentage="$3"
    local poison_num=$((n_val * percentage / 100))

    for sync_dataset_name in "${sync_dataset_names[@]}"; do
        for seed in "${seeds[@]}"; do
            for dtype in "uint64"; do
                # Calculate loss for poisoned datasets using consecutive with endpoints approach (using relaxed solution)
                echo "Processing: $sync_dataset_name n=$n_val R=$R_val seed=$seed dtype=$dtype percentage=$percentage (consecutive with endpoints using relaxed solution)"
                input_file_name="${sync_dataset_name}_n${n_val}_R${R_val}_seed${seed}_lambda${poison_num}_consecutive_w_endpoints_using_relaxed_solution_${dtype}"
                base_output_name="${sync_dataset_name}_n${n_val}_R${R_val}_seed${seed}_lambda${poison_num}_consecutive_w_endpoints_using_relaxed_solution_percentage${percentage}_${dtype}"
                run_calc_loss_consecutive_w_endpoints_using_relaxed_solution "$input_file_name" "$poison_num" "$base_output_name" "$sync_dataset_name" "$n_val"
            done
        done
    done
}

# Function to process real datasets with given parameters (consecutive with endpoints approach (duplicate allowed))
process_real_datasets_consecutive_w_endpoints_duplicate_allowed_loss() {
    local n_val="$1"
    local percentage="$2"
    local poison_num=$((n_val * percentage / 100))
    

    for real_dataset_name in "${real_dataset_names[@]}"; do
        for seed in "${seeds[@]}"; do
            for dtype in "uint64"; do
                # Calculate loss for poisoned datasets using consecutive with endpoints approach (duplicate allowed)
                echo "Processing: $real_dataset_name n=$n_val seed=$seed dtype=$dtype percentage=$percentage (consecutive with endpoints duplicate allowed)"
                input_file_name="${real_dataset_name}_n${n_val}_seed${seed}_lambda${poison_num}_consecutive_w_endpoints_duplicate_allowed_${dtype}"
                base_output_name="${real_dataset_name}_n${n_val}_seed${seed}_lambda${poison_num}_consecutive_w_endpoints_duplicate_allowed_percentage${percentage}_${dtype}"
                run_calc_loss_consecutive_w_endpoints_duplicate_allowed "$input_file_name" "$poison_num" "$base_output_name" "$real_dataset_name" "$n_val"
            done
        done
    done
}

# Function to process real datasets with given parameters (consecutive with endpoints approach (using relaxed solution))
process_real_datasets_consecutive_w_endpoints_using_relaxed_solution_loss() {
    local n_val="$1"
    local percentage="$2"
    local poison_num=$((n_val * percentage / 100))

    for real_dataset_name in "${real_dataset_names[@]}"; do
        for seed in "${seeds[@]}"; do
            for dtype in "uint64"; do
                # Calculate loss for poisoned datasets using consecutive with endpoints approach (using relaxed solution)
                echo "Processing: $real_dataset_name n=$n_val seed=$seed dtype=$dtype percentage=$percentage (consecutive with endpoints using relaxed solution)"
                input_file_name="${real_dataset_name}_n${n_val}_seed${seed}_lambda${poison_num}_consecutive_w_endpoints_using_relaxed_solution_${dtype}"
                base_output_name="${real_dataset_name}_n${n_val}_seed${seed}_lambda${poison_num}_consecutive_w_endpoints_using_relaxed_solution_percentage${percentage}_${dtype}"
                run_calc_loss_consecutive_w_endpoints_using_relaxed_solution "$input_file_name" "$poison_num" "$base_output_name" "$real_dataset_name" "$n_val"
            done
        done
    done
}

# Function to process real datasets with given parameters (duplicate allowed approach)
process_real_datasets_duplicate_allowed_loss() {
    local n_val="$1"
    local percentage="$2"
    local poison_num=$((n_val * percentage / 100))
    
    for real_dataset_name in "${real_dataset_names[@]}"; do
        for seed in "${seeds[@]}"; do
            for dtype in "uint64"; do
                # Calculate loss for poisoned datasets using duplicate allowed approach
                echo "Processing: $real_dataset_name n=$n_val seed=$seed dtype=$dtype percentage=$percentage (duplicate allowed)"
                input_file_name="${real_dataset_name}_n${n_val}_seed${seed}_lambda${poison_num}_duplicate_allowed_${dtype}"
                base_output_name="${real_dataset_name}_n${n_val}_seed${seed}_lambda${poison_num}_duplicate_allowed_percentage${percentage}_${dtype}"
                run_calc_loss_duplicate_allowed "$input_file_name" "$poison_num" "$base_output_name" "$real_dataset_name" "$n_val"
            done
        done
    done
}

# Function to process synthetic datasets with given parameters (duplicate allowed approach)
process_sync_datasets_duplicate_allowed_loss() {
    local n_val="$1"
    local R_val="$2"
    local percentage="$3"
    local poison_num=$((n_val * percentage / 100))
    
    for sync_dataset_name in "${sync_dataset_names[@]}"; do
        for seed in "${seeds[@]}"; do
            for dtype in "uint64"; do
                # Calculate loss for poisoned datasets using duplicate allowed approach
                echo "Processing: $sync_dataset_name n=$n_val R=$R_val seed=$seed dtype=$dtype percentage=$percentage (duplicate allowed)"
                input_file_name="${sync_dataset_name}_n${n_val}_R${R_val}_seed${seed}_lambda${poison_num}_duplicate_allowed_${dtype}"
                base_output_name="${sync_dataset_name}_n${n_val}_R${R_val}_seed${seed}_lambda${poison_num}_duplicate_allowed_percentage${percentage}_${dtype}"
                run_calc_loss_duplicate_allowed "$input_file_name" "$poison_num" "$base_output_name" "$sync_dataset_name" "$n_val"
            done
        done
    done
}

# Experiment 1: Vary n while keeping base_R and base_POISONING_PERCENTAGE
echo "  Experiment 1: Varying n (base_R=$base_R, base_POISONING_PERCENTAGE=$base_POISONING_PERCENTAGE)"
for n in "${ns[@]}"; do
    echo "    Varying n: $n"
    process_real_datasets_loss "$n" "$base_POISONING_PERCENTAGE"
    process_sync_datasets_loss "$n" "$base_R" "$base_POISONING_PERCENTAGE"
    process_real_datasets_duplicate_allowed_loss "$n" "$base_POISONING_PERCENTAGE"
    process_sync_datasets_duplicate_allowed_loss "$n" "$base_R" "$base_POISONING_PERCENTAGE"
    process_real_datasets_consecutive_w_endpoints_duplicate_allowed_loss "$n" "$base_POISONING_PERCENTAGE"
    process_sync_datasets_consecutive_w_endpoints_duplicate_allowed_loss "$n" "$base_R" "$base_POISONING_PERCENTAGE"
    process_real_datasets_consecutive_w_endpoints_using_relaxed_solution_loss "$n" "$base_POISONING_PERCENTAGE"
    process_sync_datasets_consecutive_w_endpoints_using_relaxed_solution_loss "$n" "$base_R" "$base_POISONING_PERCENTAGE"
done

for n in "${ns_consecutive[@]}"; do
    echo "    Varying n (for consecutive): $n"
    process_real_datasets_consecutive_loss "$n" "$base_POISONING_PERCENTAGE"
    process_sync_datasets_consecutive_loss "$n" "$base_R" "$base_POISONING_PERCENTAGE"
    process_real_datasets_consecutive_w_endpoints_loss "$n" "$base_POISONING_PERCENTAGE"
    process_sync_datasets_consecutive_w_endpoints_loss "$n" "$base_R" "$base_POISONING_PERCENTAGE"
done

for n in "${brute_force_ns[@]}"; do
    echo "    Varying n (for brute force): $n"
    process_real_datasets_loss "$n" "$base_brute_force_POISONING_PERCENTAGE"
    process_sync_datasets_loss "$n" "$base_brute_force_R" "$base_brute_force_POISONING_PERCENTAGE"
    process_real_datasets_duplicate_allowed_loss "$n" "$base_brute_force_POISONING_PERCENTAGE"
    process_sync_datasets_duplicate_allowed_loss "$n" "$base_brute_force_R" "$base_brute_force_POISONING_PERCENTAGE"
    process_real_datasets_consecutive_w_endpoints_duplicate_allowed_loss "$n" "$base_brute_force_POISONING_PERCENTAGE"
    process_sync_datasets_consecutive_w_endpoints_duplicate_allowed_loss "$n" "$base_brute_force_R" "$base_brute_force_POISONING_PERCENTAGE"
    process_real_datasets_consecutive_w_endpoints_using_relaxed_solution_loss "$n" "$base_brute_force_POISONING_PERCENTAGE"
    process_sync_datasets_consecutive_w_endpoints_using_relaxed_solution_loss "$n" "$base_brute_force_R" "$base_brute_force_POISONING_PERCENTAGE"
done

for n in "${ns_consecutive[@]}"; do
    echo "    Varying n (for consecutive): $n"
    process_real_datasets_consecutive_loss "$n" "$base_POISONING_PERCENTAGE"
    process_sync_datasets_consecutive_loss "$n" "$base_R" "$base_POISONING_PERCENTAGE"
    process_real_datasets_consecutive_w_endpoints_loss "$n" "$base_POISONING_PERCENTAGE"
    process_sync_datasets_consecutive_w_endpoints_loss "$n" "$base_R" "$base_POISONING_PERCENTAGE"
done

# Experiment 2: Vary R while keeping base_n and base_POISONING_PERCENTAGE
echo "  Experiment 2: Varying R (base_n=$base_n, base_POISONING_PERCENTAGE=$base_POISONING_PERCENTAGE)"
for R in "${Rs[@]}"; do
    if [ "$R" -le "$base_n" ]; then
        continue
    fi
    echo "    Varying R: $R"
    process_sync_datasets_loss "$base_n" "$R" "$base_POISONING_PERCENTAGE"
    process_sync_datasets_duplicate_allowed_loss "$base_n" "$R" "$base_POISONING_PERCENTAGE"
    process_sync_datasets_consecutive_w_endpoints_duplicate_allowed_loss "$base_n" "$R" "$base_POISONING_PERCENTAGE"
    process_sync_datasets_consecutive_w_endpoints_using_relaxed_solution_loss "$base_n" "$R" "$base_POISONING_PERCENTAGE"
done

for R in "${Rs_consecutive[@]}"; do
    echo "    Varying R (for consecutive): $R"
    process_sync_datasets_consecutive_loss "$base_n" "$R" "$base_POISONING_PERCENTAGE"
    process_sync_datasets_consecutive_w_endpoints_loss "$base_n" "$R" "$base_POISONING_PERCENTAGE"
done

for R in "${brute_force_Rs[@]}"; do
    if [ "$R" -le "$base_brute_force_n" ]; then
        continue
    fi
    echo "    Varying R (for brute force): $R"
    process_sync_datasets_loss "$base_brute_force_n" "$R" "$base_brute_force_POISONING_PERCENTAGE"
    process_sync_datasets_duplicate_allowed_loss "$base_brute_force_n" "$R" "$base_brute_force_POISONING_PERCENTAGE"
done

for R in "${Rs_consecutive[@]}"; do
    echo "    Varying R (for consecutive): $R"
    process_sync_datasets_consecutive_loss "$base_brute_force_n" "$R" "$base_brute_force_POISONING_PERCENTAGE"
    process_sync_datasets_consecutive_w_endpoints_loss "$base_brute_force_n" "$R" "$base_brute_force_POISONING_PERCENTAGE"
    process_sync_datasets_consecutive_w_endpoints_duplicate_allowed_loss "$base_brute_force_n" "$R" "$base_brute_force_POISONING_PERCENTAGE"
    process_sync_datasets_consecutive_w_endpoints_using_relaxed_solution_loss "$base_brute_force_n" "$R" "$base_brute_force_POISONING_PERCENTAGE"
done


# Experiment 3: Varying poisoning percentage while keeping base_n and base_R
echo "  Experiment 3: Varying poisoning percentage (base_n=$base_n, base_R=$base_R)"
for percentage in "${POISONING_PERCENTAGES[@]}"; do
    echo "    Varying poisoning percentage: $percentage"
    process_real_datasets_loss "$base_n" "$percentage"
    process_sync_datasets_loss "$base_n" "$base_R" "$percentage"
    process_real_datasets_duplicate_allowed_loss "$base_n" "$percentage"
    process_sync_datasets_duplicate_allowed_loss "$base_n" "$base_R" "$percentage"
    process_real_datasets_consecutive_w_endpoints_duplicate_allowed_loss "$base_n" "$percentage"
    process_sync_datasets_consecutive_w_endpoints_duplicate_allowed_loss "$base_n" "$base_R" "$percentage"
    process_real_datasets_consecutive_w_endpoints_using_relaxed_solution_loss "$base_n" "$percentage"
    process_sync_datasets_consecutive_w_endpoints_using_relaxed_solution_loss "$base_n" "$base_R" "$percentage"
done

for percentage in "${POISONING_PERCENTAGES_consecutive[@]}"; do
    echo "    Varying poisoning percentage (for consecutive): $percentage"
    process_real_datasets_consecutive_loss "$base_n" "$percentage"
    process_sync_datasets_consecutive_loss "$base_n" "$base_R" "$percentage"
    process_real_datasets_consecutive_w_endpoints_loss "$base_n" "$percentage"
    process_sync_datasets_consecutive_w_endpoints_loss "$base_n" "$base_R" "$percentage"
done

for percentage in "${brute_force_POISONING_PERCENTAGES[@]}"; do
    echo "    Varying poisoning percentage (for brute force): $percentage"
    process_real_datasets_loss "$base_brute_force_n" "$percentage"
    process_sync_datasets_loss "$base_brute_force_n" "$base_brute_force_R" "$percentage"
    process_real_datasets_duplicate_allowed_loss "$base_brute_force_n" "$percentage"
    process_sync_datasets_duplicate_allowed_loss "$base_brute_force_n" "$base_brute_force_R" "$percentage"
    process_real_datasets_consecutive_w_endpoints_duplicate_allowed_loss "$base_brute_force_n" "$percentage"
    process_sync_datasets_consecutive_w_endpoints_duplicate_allowed_loss "$base_brute_force_n" "$base_brute_force_R" "$percentage"
    process_real_datasets_consecutive_w_endpoints_using_relaxed_solution_loss "$base_brute_force_n" "$percentage"
    process_sync_datasets_consecutive_w_endpoints_using_relaxed_solution_loss "$base_brute_force_n" "$base_brute_force_R" "$percentage"
done

for percentage in "${POISONING_PERCENTAGES_consecutive[@]}"; do
    echo "    Varying poisoning percentage (for consecutive): $percentage"
    process_real_datasets_consecutive_loss "$base_brute_force_n" "$percentage"
    process_sync_datasets_consecutive_loss "$base_brute_force_n" "$base_brute_force_R" "$percentage"
    process_real_datasets_consecutive_w_endpoints_loss "$base_brute_force_n" "$percentage"
    process_sync_datasets_consecutive_w_endpoints_loss "$base_brute_force_n" "$base_brute_force_R" "$percentage"
done

echo "Loss calculation completed"
echo "  Results saved to: $RESULTS_DIR/loss/"
echo "  Consecutive results saved to: $RESULTS_DIR/loss_consecutive/"
echo "  Consecutive with endpoints results saved to: $RESULTS_DIR/loss_consecutive_w_endpoints/"
echo "  Consecutive with endpoints duplicate allowed results saved to: $RESULTS_DIR/loss_consecutive_w_endpoints_duplicate_allowed/"
echo "  Duplicate allowed results saved to: $RESULTS_DIR/loss_duplicate_allowed/"
echo ""

# Count JSON files for all approaches
total_loss_files=$(find "$RESULTS_DIR/loss/" -name "*.json" -type f | wc -l)
total_consecutive_files=$(find "$RESULTS_DIR/loss_consecutive/" -name "*.json" -type f | wc -l)
total_consecutive_w_endpoints_files=$(find "$RESULTS_DIR/loss_consecutive_w_endpoints/" -name "*.json" -type f | wc -l)
total_consecutive_w_endpoints_duplicate_allowed_files=$(find "$RESULTS_DIR/loss_consecutive_w_endpoints_duplicate_allowed/" -name "*.json" -type f | wc -l)
total_duplicate_allowed_files=$(find "$RESULTS_DIR/loss_duplicate_allowed/" -name "*.json" -type f | wc -l)
total_consecutive_w_endpoints_using_relaxed_solution_files=$(find "$RESULTS_DIR/loss_consecutive_w_endpoints_using_relaxed_solution/" -name "*.json" -type f | wc -l)
echo "    Total loss result files: $total_loss_files"
echo "    Total consecutive loss result files: $total_consecutive_files"
echo "    Total consecutive with endpoints loss result files: $total_consecutive_w_endpoints_files"
echo "    Total consecutive with endpoints duplicate allowed loss result files: $total_consecutive_w_endpoints_duplicate_allowed_files"
echo "    Total duplicate allowed loss result files: $total_duplicate_allowed_files"
echo "    Total consecutive with endpoints using relaxed solution result files: $total_consecutive_w_endpoints_using_relaxed_solution_files"
echo ""